 Website Parser

## Анализ задачи

### Что было сделано
В этом проекте была разработана программа для парсинга сайта https://quotes.toscrape.com/ с целью извлечения цитат и данных авторов цитат.
0. **Проверка доступности сайта**: Убедился, что сайт доступен и после запроса сервер возвращает код ответа 200. Далее убедился на отсутствие CAPCHA или защиты от ботов Cloudfare. 
1. **Анализ исходных данных**: Просмотрел отрендеренную версию сайта, структура сайта оказалась довольно однотипной. Далее начал анализировать разметку страницы, оказалось что все карточки с контентом имеют одинаковые теги, классы и одну и ту же структуру. Принял решение собирать контент из карточек. Также обнаружил, что переход на следующие страницы осуществлялся однотипно.
2. **Постановка задачи**: В каждой карточке используются HTML-теги с одинаковыми классами из которых требуется извлечь контент, сохраняя структуру карточки. В каждой карточке находится URL на страницу с данными об авторах. Эти данные требуется извлечь.  Для автоматизации парсинга всего сайта, требуется автоматический переход на следующую страницу. Требуется сохранить данные и после экспортировать их в JSON-файл.


### Откуда были получены данные
Данные были собраны со всех страниц сайта.
После каждой загрузки HTML-страницы происходил поиск тегов с определёнными классами из которых выгружался контент до того момента, когда страниц для анализа больше не останется. Во время поиска, в каждой карточке был URL на автора. Программа автоматически переходила по найденному URL и собирала данные об авторе.

### Как осуществлялся сбор
Сбор данных осуществлялся с помощью следующих этапов:
1. **HTTP-запросы**: Использовалась утилита curl. После выполнения команды перенаправлял поток вывода данных в парсер. Данные состоят из HTML-страницы.
2. **Парсинг HTML**: Для извлечения контента использовал библиотеку `BeautifulSoup`, После анализа исходных данных, написал функцию-генератор (для экономии памяти) для поиска контента, в которой указывал теги и классы содержащие в себе контент. При исполнении функции начинается поиск карточки и ссылки на следующую страницу. Если карточка нашлась, то из неё извлекаются имя автора, текст цитаты, названия тегов. Если в карточке присутствует URL на страницу автора, осуществляется переход и сбор данных автора (имя, данные о рождении, описание автора).
3. **Сохранение данных**: После парсинга, данные сохраняются в базу данных. Создаются таблицы с именами авторов, цитатами, тегами цитаты, описание авторов и данные их рождения. Таблица с именами авторов содержит информацию об авторе (имя, место рождения, описание). Таблица с тегами содержит названия тегов, к которым могут принадлежать цитаты. Таблица цитат содержит тексты цитат и ссылку на автора, Промежуточная таблица для связи цитат и тегов.
4. **Экспорт данных в JSON-файл**: Из базы данных извлекаются данные об авторах, их цитаты, связанные теги. Создаётся пустой список, выбираются все авторы, для каждого автора создаётся словарь с его именем и цитатами с тегами. Словарь добавляется в список. Для следующих авторов процедура аналогична. В конце этот список записывается в JSON-файл с исходной кодировкой и удобным форматом. Также экспортируются данные об авторах (имя, место рождения, описание). Создаётся пустой список, выбирается таблица авторов, создаётся словарь в которой содержится имя автора, место рождения, описание. В конце этот список записывается в JSON-файл с исходной кодировкой и удобным форматом.

### Почему был выбран тот или иной метод/инструмент, а не другой
1. **Curl**: Потребляет мало ресурсов машины при массовых запросах, позволяет выполнять последовательные команды в сложных автоматизированных процессах. Не ограничена только HTTP/HTTPS как requests. Имеет понятный синтаксис.
2. **BeautifulSoup**: Имеет инструменты поиска по CSS-селекторам и регулярным выражениям, что позволяет быстро находить элементы по тегам, атрибутам и содержимому. Справляется с невалидным HTML-кодом (например, с ошибками, не закрытыми тегами или вложениями). Поддерживает несколько парсеров, включая html.parser (стандартный парсер Python), lxml и html5lib, что даёт гибкость в выборе. Простой синтаксис, не требует много ресурсов машины.
3. **Peewee**: Поддерживает работу с SQLite, MySQL и PostgreSQL, обеспечивая переносимость кода между этими базами данных. Автоматически управляет подключением к базе данных. Из-за простоты и меньшего количества абстракций, работает быстрее по сравнению с более крупными ORM, такими как Django ORM или SQLAlchemy, особенно при работе с небольшими объемами данных. Легковесный, простой в использовании. 
4. **SQLite**: Не требует сервера, что упрощает её настройку и эксплуатацию. Все данные хранятся в одном файле, который можно легко перемещать или архивировать вместе с проектом. Потребляет меньше ресурсов, чем серверные СУБД, и может эффективно работать на машинах с ограниченными вычислительными возможностями.

